# -*- coding: utf-8 -*-
"""DP Lab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EDWKjoPTyuaWvk6-M3_lLWhYedE4jXH6
"""

import numpy as np

def sigmoid(x):
    """Sigmoid activation function"""
    return 1 / (1 + np.exp(-x))

def predict(inputs, weights, bias):
    """Compute the output of a single artificial neuron"""
    # Weighted sum (z = w1*x1 + w2*x2 + ... + bias)
    z = np.dot(inputs, weights) + bias
    # Apply activation function
    output = sigmoid(z)
    return output

# Inputs (example data)
# Each row is an input example (e.g., [x1, x2])
X = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

# Target outputs (labels)
y = np.array([0, 0, 0, 1])  # AND gate behavior

# Initialize weights and bias
weights = np.array([1.0, 1.0])  # Weight for each input
bias = -1.5  # Bias term

# Perform predictions
outputs = []
for x in X:
    output = predict(x, weights, bias)
    binary_output = 1 if output >= 0.5 else 0  # Binary classification (threshold = 0.5)
    outputs.append(binary_output)

# Display results
print("Inputs:\n", X)
print("Weights:", weights)
print("Bias:", bias)
print("Predicted Outputs:", outputs)

import numpy as np

# Step 1: Define the AND gate dataset
# Inputs: x1, x2 (binary values)
# Output: y (binary output)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features
y = np.array([0, 0, 0, 1])  # Output labels for AND gate

# Step 2: Initialize weights and bias
weights = np.zeros(X.shape[1])  # One weight for each input
bias = 0
learning_rate = 0.1
epochs = 10  # Number of training epochs

# Step 3: Define the activation function (Step function)
def step_function(x):
    return 1 if x >= 0 else 0

# Step 4: Training the Perceptron
for epoch in range(epochs):
    for i in range(len(X)):
        # Compute the linear combination
        linear_output = np.dot(X[i], weights) + bias
        # Apply the step function to get the prediction
        prediction = step_function(linear_output)
        # Update weights and bias using the Perceptron rule
        weights += learning_rate * (y[i] - prediction) * X[i]
        bias += learning_rate * (y[i] - prediction)
    print(f'Epoch {epoch+1}: Weights: {weights}, Bias: {bias}')

# Step 5: Testing the trained model
print("\nTesting the trained model:")
for i in range(len(X)):
    linear_output = np.dot(X[i], weights) + bias
    prediction = step_function(linear_output)
    print(f'Input: {X[i]}, Prediction: {prediction}, Actual: {y[i]}')

import numpy as np

# Step 1: Define the XOR dataset
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features
y = np.array([[0], [1], [1], [0]])  # Output labels for XOR gate

# Step 2: Initialize parameters
input_size = X.shape[1]  # 2 input features
hidden_size = 2          # Number of hidden layer neurons
output_size = 1          # Single output
learning_rate = 0.1
epochs = 10000

# Initialize weights and biases
weights_input_hidden = np.random.randn(input_size, hidden_size)
weights_hidden_output = np.random.randn(hidden_size, output_size)
bias_hidden = np.zeros((1, hidden_size))
bias_output = np.zeros((1, output_size))

# Step 3: Define the activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Step 4: Training the MLP
for epoch in range(epochs):
    # Forward pass
    hidden_layer_input = np.dot(X, weights_input_hidden) + bias_hidden
    hidden_layer_output = sigmoid(hidden_layer_input)

    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output
    predicted_output = sigmoid(output_layer_input)

    # Compute the error (difference between predicted and actual)
    error = y - predicted_output

    # Backpropagation
    d_predicted_output = error * sigmoid_derivative(predicted_output)
    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)
    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)

    # Update the weights and biases
    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate
    bias_output += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate
    weights_input_hidden += X.T.dot(d_hidden_layer) * learning_rate
    bias_hidden += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate

    if epoch % 1000 == 0:  # Print error every 1000 epochs
        print(f'Epoch {epoch} Error: {np.mean(np.abs(error))}')

# Step 5: Testing the trained model
print("\nTesting the trained model:")
for i in range(len(X)):
    hidden_layer_input = np.dot(X[i], weights_input_hidden) + bias_hidden
    hidden_layer_output = sigmoid(hidden_layer_input)
    output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output
    prediction = sigmoid(output_layer_input)
    print(f'Input: {X[i]}, Predicted: {prediction.round()}, Actual: {y[i]}')

import numpy as np
import matplotlib.pyplot as plt

# Define the activation functions
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def relu(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)

# Create input data (from -10 to 10)
x = np.linspace(-10, 10, 100)

# Apply the activation functions to the input data
sigmoid_output = sigmoid(x)
relu_output = relu(x)
tanh_output = tanh(x)

# Plot the outputs of the activation functions
plt.figure(figsize=(8, 6))

# Plot Sigmoid
plt.subplot(3, 1, 1)
plt.plot(x, sigmoid_output, label="Sigmoid", color='blue')
plt.title("Sigmoid Activation Function")
plt.xlabel("Input")
plt.ylabel("Output")
plt.grid(True)

# Plot ReLU
plt.subplot(3, 1, 2)
plt.plot(x, relu_output, label="ReLU", color='green')
plt.title("ReLU Activation Function")
plt.xlabel("Input")
plt.ylabel("Output")
plt.grid(True)

# Plot Tanh
plt.subplot(3, 1, 3)
plt.plot(x, tanh_output, label="Tanh", color='red')
plt.title("Tanh Activation Function")
plt.xlabel("Input")
plt.ylabel("Output")
plt.grid(True)

# Show the plots
plt.tight_layout()
plt.show()

import numpy as np

# Sigmoid activation function and its derivative
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

# Mean Squared Error loss function and its derivative
def mse_loss(y_true, y_pred):
    return ((y_true - y_pred) ** 2).mean()

# Dataset (XOR problem)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data
y = np.array([[0], [1], [1], [0]])  # Expected outputs

# Initialize weights and biases for the 2-layer neural network
input_size = 2  # Number of input neurons
hidden_size = 2  # Number of hidden neurons
output_size = 1  # Number of output neurons

# Random weights initialization (between input and hidden layer, and hidden and output layer)
np.random.seed(42)  # For reproducibility
weights_input_hidden = np.random.randn(input_size, hidden_size)
weights_hidden_output = np.random.randn(hidden_size, output_size)

# Bias initialization
bias_hidden = np.zeros((1, hidden_size))
bias_output = np.zeros((1, output_size))

# Learning rate and number of iterations
learning_rate = 0.1
epochs = 10000

# Training loop
for epoch in range(epochs):
    # Forward Propagation
    # Input to hidden layer
    hidden_input = np.dot(X, weights_input_hidden) + bias_hidden
    hidden_output = sigmoid(hidden_input)  # Apply sigmoid activation

    # Hidden to output layer
    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output
    output = sigmoid(output_input)  # Apply sigmoid activation

    # Loss calculation (Mean Squared Error)
    loss = mse_loss(y, output)

    # Backpropagation
    # Output layer error
    output_error = y - output  # The difference between actual and predicted output
    output_delta = output_error * sigmoid_derivative(output)  # Derivative of sigmoid

    # Hidden layer error
    hidden_error = output_delta.dot(weights_hidden_output.T)  # Backpropagate the error
    hidden_delta = hidden_error * sigmoid_derivative(hidden_output)  # Derivative of sigmoid

    # Update weights and biases
    weights_hidden_output += hidden_output.T.dot(output_delta) * learning_rate
    bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate
    weights_input_hidden += X.T.dot(hidden_delta) * learning_rate
    bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate

    # Print the loss at each 1000th epoch
    if epoch % 1000 == 0:
        print(f'Epoch {epoch}, Loss: {loss}')

# Testing the trained model
print("\nTesting the trained model:")
for i in range(len(X)):
    hidden_input = np.dot(X[i], weights_input_hidden) + bias_hidden
    hidden_output = sigmoid(hidden_input)
    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output
    prediction = sigmoid(output_input)
    print(f'Input: {X[i]}, Predicted: {prediction.round()}, Actual: {y[i]}')